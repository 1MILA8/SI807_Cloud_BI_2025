# ğŸ“ **ExtracciÃ³n, TransformaciÃ³n y Carga (Bronce)**

AquÃ­ se explica los pipelines responsables de la **ingesta, limpieza y estandarizaciÃ³n inicial** de los datos provenientes de fuentes externas (SBS, BCRP). Representa la **capa Bronce** dentro de la arquitectura Medallion.

Los ETL se ejecutan a travÃ©s de una **Cloud Function (bronce-dispatcher)** que detecta automÃ¡ticamente nuevos archivos subidos al bucket de Google Cloud Storage.

---

## ğŸ“Œ **Recordatorio: CÃ³digo Fuente de la Cloud Function**

El cÃ³digo completo de la Cloud Function se encuentra en la carpeta:

ğŸ‘‰ **`/resources/bronce-dispatcher/`** ([Ver carpeta en GitHub](../resources)) 

### Estructura interna de la Cloud Function:

```
resources/
â””â”€â”€ bronce-dispatcher
    â”œâ”€â”€ config
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ paths.py
    â”œâ”€â”€ pipelines                (ETL) <- AQUÃ ESTAMOS
    â”‚   â”œâ”€â”€ bcrp
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ tipo_cambio.py
    â”‚   â””â”€â”€ sbs
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ creditos.py
    â”‚       â”œâ”€â”€ depositos.py
    â”‚       â”œâ”€â”€ patrimonio.py
    â”‚       â””â”€â”€ ratio_liquidez.py
    â”œâ”€â”€ utils
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ bigquery.py
    â”‚   â””â”€â”€ gcs.py
    â”œâ”€â”€ main.py
    â”œâ”€â”€ requirements.txt
```

ğŸ¯ **Ahora nos encontramos explicando el Pipeline de ETL**

---

## âœ… **Flujo General del ETL**

### **1ï¸âƒ£ Ingesta del archivo en el Data Lake (Raw)**

Los archivos se suben a:

```
gs://grupo6_scotiabank_bucket/data/raw/<FUENTE>/<ENTIDAD>/
```

Ejemplos:

* `data/raw/SBS/RATIO_LIQUIDEZ/`
* `data/raw/SBS/DEPOSITOS/`
* `data/raw/BCRP/TIPO_CAMBIO/`

Cada subida genera un evento GCS â†’ Cloud Function.

![Bucket GCS](Evidencias/grupo6_scotiabank_bucket.png)

---

### **2ï¸âƒ£ Dispatcher de Cloud Function**

La funciÃ³n `bronce-dispatcher` identifica quÃ© pipeline ejecutar segÃºn el prefijo del archivo.

Ejemplo:

```python
if name.startswith(paths.PREFIX_RATIO_LIQUIDEZ):
    run_pipeline_ratio_liquidez(bucket, name)
```

---

### **3ï¸âƒ£ TransformaciÃ³n**

Cada pipeline:

âœ” Lee Excel/CSV desde GCS (Extraction)
âœ” Limpia y normaliza **(Transformation)**  
âœ” Estandariza columnas **(Transformation)**  
âœ” Convierte a formato tabular uniforme **(Transformation)**  
âœ” AÃ±ade campos de auditorÃ­a **(Transformation)**   
âœ” Inserta en BigQuery (Bronce) (Load)

---

#### ğŸ§© **Ejemplo Detallado: Pipeline ETL - Ratio de Liquidez**

![Ratio de liquidez](Evidencias/raw_ratio_liquidez.png)


##### ğŸ” **Lectura del archivo**

```python
df_raw = leer_excel_desde_gcs(bucket_name, file_name, "21")
```

Lee una hoja especÃ­fica (â€œ21â€) del Excel SBS.

---

##### ğŸ—‚ **ExtracciÃ³n de metadatos desde el nombre del archivo**

Se obtiene **mes y aÃ±o** automÃ¡ticamente:

```python
parts = file_name.replace(".xlsx", "").split("-")
mes = raw_date[:2]
anio = int(raw_date[-4:])
```

---

##### ğŸ”§ **Procesamiento de bloques MN y ME**

El Excel contiene:

* Bloque en **Moneda Nacional (MN)**
* Bloque en **Moneda Extranjera (ME)**

Se separan y combinan:

```python
df_total = pd.concat([mn, me], ignore_index=True)
```

---

##### ğŸ§¹ **Limpieza de datos**

* RemociÃ³n de comas
* RemociÃ³n de signos de porcentaje
* Trimming

```python
df_total[col] = df_total[col].str.replace(",", "").str.replace("%", "")
```

---

##### ğŸ· **Columnas de auditorÃ­a**

```python
df_total["anio"] = anio
df_total["mes"] = mes
df_total["fecha_carga"] = datetime.utcnow()
```

---

## **4ï¸âƒ£ Load: Carga a BigQuery - Capa Bronce**

```python
cargar_dataframe_bigquery(df_transformado, DATASET_BRONCE, TABLE_RATIO)
```

El dataset utilizado es:

```
bronce.sbs_liquidez
```

---

## **5ï¸âƒ£ Lanzamiento AutomÃ¡tico del Job Dataproc (Plata â†’ Oro)**

DespuÃ©s de cargar a Bronce, se lanza un job PySpark:

```python
ejecutar_dataproc_ratio_liquidez()
```

Este job:

âœ” Lee `bronce.*`
âœ” Genera tablas **plata.sbs_*
âœ” Construye tablas oro (`hecho_riesgo`, dimensiones, cÃ¡lculos)

Esto se explique a detalle en ([05_Procesamiento_Spark](../05_Procesamiento_Spark/README.md)) 

---

# ğŸ¥ **ExplicaciÃ³n del proceso**

Si quieres ver el proceso detallado, puedes ver el siguiente video:

[![Video explicativo](https://img.youtube.com/vi/jdcOAb0z28s/maxresdefault.jpg)](https://youtu.be/jdcOAb0z28s)