# üìÅ **/resources - Cloud Function y Jobs PySpark**

La carpeta **`/resources`** contiene todos los componentes que permiten automatizar el procesamiento de datos dentro de la arquitectura Medallion (Bronce ‚Üí Plata ‚Üí Oro).

Aqu√≠ se encuentran:

* La **Cloud Function `bronce-dispatcher`**, que se activa ante nuevos archivos RAW en Cloud Storage.
* Los **jobs PySpark ejecutados en Dataproc**, encargados de procesar la capa Bronce, Plata y generar la capa Oro.
* El job especial `crear_capa_oro.py`, que inicializa las **dimensiones del modelo anal√≠tico Oro**.
* **Video explicativo** del proceso.

---

# üóÇÔ∏è **Estructura del directorio**

```
resources/
‚îú‚îÄ‚îÄ bronce-dispatcher/        # Cloud Function (ETL Bronce + orquestaci√≥n)
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ paths.py          # Prefijos, rutas y configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/            # Pipelines ETL por entidad
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bcrp/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tipo_cambio.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sbs/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ creditos.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ depositos.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ patrimonio.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ratio_liquidez.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/                # Lectura GCS y carga BigQuery
‚îÇ   ‚îú‚îÄ‚îÄ main.py               # Dispatcher (entry point)
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îî‚îÄ‚îÄ jobs/                     # Scripts PySpark enviados a Dataproc
    ‚îú‚îÄ‚îÄ crear_capa_oro.py     # Crea tablas DIM y estructura Oro
    ‚îú‚îÄ‚îÄ jb_creditos.py
    ‚îú‚îÄ‚îÄ jb_depositos.py
    ‚îú‚îÄ‚îÄ jb_patrimonio.py
    ‚îú‚îÄ‚îÄ jb_ratio_liquidez.py
    ‚îî‚îÄ‚îÄ jb_tipo_cambio.py
```

---

# ‚òÅÔ∏è **Cloud Function: `bronce-dispatcher`**

La Cloud Function implementa el mecanismo autom√°tico de ingesta:

## üîÑ ¬øC√≥mo funciona?

1. Detecta un archivo nuevo en el bucket:

   ```
   gs://grupo6_scotiabank_bucket/data/raw/...
   ```
2. Identifica el pipeline seg√∫n el prefijo (ratio, dep√≥sitos, cr√©ditos, patrimonio‚Ä¶)
3. Llama al pipeline correspondiente:

   * Lee Excel/CSV
   * Limpia y transforma
   * Carga a BigQuery (Bronce)
4. Lanza autom√°ticamente un **job PySpark (Dataproc)** para procesar la capa Plata ‚Üí Oro.

### Vista del bucket GCS

![Bucket GCS](Evidencias/grupo6_scotiabank_bucket.png)

---

# üöÄ **Despliegue de la Cloud Function**

Cuando se hacen cambios en `bronce-dispatcher`, la funci√≥n se despliega a GCP con:

```bash
gcloud functions deploy bronce-dispatcher \
  --gen2 \
  --runtime python310 \
  --region southamerica-west1 \
  --source=. \
  --entry-point bronce_dispatcher \
  --trigger-bucket grupo6_scotiabank_bucket \
  --memory 512MB
```

### Subida de la Cloud Function

![Subir Cloud Function](Evidencias/Subir_cloud_fuction.png)

### Cloud Function cargada

![Cloud Function cargando](Evidencias/Cargando_cloud_fuction.png)

> Este comando empaqueta toda la carpeta `bronce-dispatcher/` y la sube a GCP.

---


# üî• **Jobs PySpark en Dataproc**

Cada pipeline ETL (Bronce) dispara autom√°ticamente un job PySpark que procesa:

* **Plata:** limpieza profunda y armonizaci√≥n
* **Oro:** construcci√≥n del modelo anal√≠tico

Los jobs se encuentran en:

```
/resources/jobs/
```

### Los jobs disponibles son:

| Job                     | Rol                                                |
| ----------------------- | -------------------------------------------------- |
| `jb_ratio_liquidez.py`  | Procesa ratio de liquidez                          |
| `jb_depositos.py`       | Procesa dep√≥sitos SBS                              |
| `jb_creditos.py`        | Procesa cr√©ditos SBS                               |
| `jb_patrimonio.py`      | Procesa patrimonio SBS                             |
| `jb_tipo_cambio.py`     | Procesa tipo de cambio BCRP                        |
| **`crear_capa_oro.py`** | *Job especial: crea dimensiones de la capa Oro* |

---

# üèóÔ∏è **Job especial: Crear estructura de la Capa Oro**

Antes de ejecutar cualquier job anal√≠tico, es necesario crear:

‚úî `dim_fecha`
‚úî `dim_banco`
‚úî `dim_moneda`
‚úî `dim_indicador`
‚úî Estructura base de `hecho_riesgo`

Este proceso se ejecuta una vez o cuando se quiera reiniciar la capa oro.

---

## ‚ñ∂Ô∏è Ejecuci√≥n del job para crear la Capa Oro

Usa el siguiente comando:

```bash
gcloud dataproc batches submit pyspark \
  gs://grupo6_scotiabank_bucket/resources/crear_capa_oro.py \
  --project=grupo6-scotiabank \
  --region=southamerica-west1 \
  --batch=create-oro-tables \
  --version=2.1 \
  --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar \
  --properties=spark.executor.instances=2,spark.executor.cores=4,spark.executor.memory=4g,spark.driver.cores=4,spark.driver.memory=4g
```

### Cargando job capa oro

![Cargando job oro](Evidencias/Cargando_job_capa_oro.png)

### Ejecutando job capa oro

![Ejecutar job oro](Evidencias/Ejecutar_job_capa_oro.png)

---



# üé¨ **Explicaci√≥n del proceso**

Si quieres ver el proceso detallado, puedes ver el siguiente video:

[![Video explicativo](https://img.youtube.com/vi/IdxF6J1P_q0/maxresdefault.jpg)](https://youtu.be/IdxF6J1P_q0)