#  ğŸ¦ Proyecto BI Scotiabank 
## ğŸ—ï¸ PC4 â€“ Arquitectura AnalÃ­tica en la Nube con Google Cloud Platform
El objetivo de este trabajo es implementar un flujo funcional de sistema de inteligencia de Negocio en la nube para la empresa Scotiabank que permitira:
- âœ” Obtener datos desde la web
- âœ” Procesarlos y almacenarlos en un Data Lake multicapa
- âœ” Transformarlos con PySpark
- âœ” Consolidar informaciÃ³n en BigQuery
- âœ” Generar un esquema estrella en BigQuery
- âœ” Visualizar dashboards en Power BI

### Estructura de Carpetas

```json
/PC4
 â”œâ”€â”€ README.md                 # DocumentaciÃ³n principal (este archivo)
 â”œâ”€â”€ 01_Ambiente_GCP/          # CreaciÃ³n del proyecto, IAM, VPC, buckets, claves
 â”œâ”€â”€ 02_WebScraping/           # CÃ³digo Python para extracciÃ³n de datos
 â”œâ”€â”€ 03_ETL/                   # Limpieza, consolidaciÃ³n, CSV, carga autom.
 â”œâ”€â”€ 04_DataLake/              # Raw / Trusted / Refined en GCS
 â”œâ”€â”€ 05_Procesamiento_Spark/   # Dataproc, PySpark, notebooks
 â”œâ”€â”€ 06_BigQuery/              # Tablas, consultas SQL
 â”œâ”€â”€ 07_PowerBI/               # ConexiÃ³n con BigQuery + dashboards
 â”œâ”€â”€ Evidencias_Generales/     # Capturas, videos, PR, merges
 â””â”€â”€ docs/                     
```
## ğŸ§± 1. Arquitectura Avanzada en la Nube

Arquitectura desplegada sobre Google Cloud Platform (GCP) en el proyecto:

```
ID del Proyecto: grupo6-scotiabank
```

La soluciÃ³n integra servicios bÃ¡sicos, avanzados y complementarios para soportar un flujo completo de analÃ­tica de datos, desde la ingesta hasta el consumo en Power BI.

### âœ”ï¸ Servicios utilizados y evidencias de costos

Durante la implementaciÃ³n se hizo uso real de los siguientes servicios en la nube (monto total invertido demostrado en gastos de GCP):

| Servicio                 | Costo (S/.) | Rol dentro de la Arquitectura                                       |
|--------------------------|-------------|----------------------------------------------------------------------|
| **Networking**           | 1.19        | ComunicaciÃ³n segura entre servicios, API y rutas privadas            |
| **BigQuery**             | 3.37        | Almacenamiento analÃ­tico, consultas SQL, datasets trusted/refined    |
| **BigQuery Reservation** | 12.64       | Reserva de slots para consultas de alto rendimiento                  |
| **Dataproc**             | 21.27       | Procesamiento distribuido con PySpark                                |
| **Compute Engine**       | 6.37        | Nodo de soporte/worker para ejecuciÃ³n puntual                        |
| **Cloud Storage**        | 0.30        | Data Lake multicapa: raw â†’ trusted â†’ refined                         |
| **Cloud Run**            | 18.76       | Servicios serverless para tareas auxiliares y componentes            |
| **Cloud Run Functions**  | 0.10        | Funciones event-driven para automatizaciÃ³n                           |
| **Cloud Build**          | 0.00        | ConstrucciÃ³n automÃ¡tica de artefactos                                |

![1-facturacion_Servicios](Evidencias_generales\1-Facturacion_Actual.png)


### âœ”ï¸ Storage estructurado (raw / trusted / refined)

La estrutura actual de Bucket en Cloud Storage tiene la siguiente forma

![alt text](Evidencias_Generales/4-Bukckets.png)

El Bucket Principal como Data Lake es el de **"grupo6_scotiabank_bucket"**.

Por otro lado se tiene Buckets adicionales generados automÃ¡ticamente por los servicios y recursos utilizados:

- **dataproc-temp-southamerica-west1-..Dataproc** : Ãrea temporal donde Dataproc guarda metadatos, logs, intermediarios y staging

- **gcf-v2-sources-75587073872-southamerica-west1	Cloud Run / Cloud Functions** : Almacena el cÃ³digo fuente desplegado por funciones y servicios serverless, permitiendo versionamiento y redeploy

El Data Lake se organizÃ³ bajo la estructura recomendada para arquitecturas analÃ­ticas:
```
gs://grupo6_scotiabank_bucket/   
â”‚   â”œâ”€â”€ Capa Bronce/ # Datos originales tal como se ingresa (Capa Bronce, data/raw/SBS)
â”œâ”€â”€â”€â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ historico/  #Archivos Consolidados
â”‚   â”‚   â””â”€â”€ raw/   # Datos Extraidos del Web Scraping
â”‚   â”‚       â””â”€â”€ SBS/
â”‚   â”‚           â”œâ”€â”€ CREDITOS_SEGUN_SITUACION/
â”‚   â”‚           â”œâ”€â”€ DEPOSITOS/
â”‚   â”‚           â”œâ”€â”€ EEFF/
â”‚   â”‚           â”œâ”€â”€ PATRIMONIO_EFECTIVO/
â”‚   â”‚           â”œâ”€â”€ PATRIMONIO_REQUERIDO_RCG/
â”‚   â”‚           â””â”€â”€ RATIO_LIQUIDEZ/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ resources/
â”œâ”€â”€â”€â””â”€â”€ Capa Plata/ # Datos curados, limpios y tipificados (Capa Plata, trusted)
â””â”€â”€â”€â””â”€â”€ Capa Oro/   # Datos listos para explotaciÃ³n analÃ­tica (Power BI / BigQuery, refined)
    
```

![Buckets_Data_Lake](Evidencias_Generales/5-Buckets_Data_Lake.png)

ğŸ‘‰ Este reparto permite separar la capa principal de datos del cÃ³digo operacional, garantizando gobernanza y control de versiones.


### âœ”ï¸ Visor BI en la nube

El consumo analÃ­tico se realiza mediante:

Power BI conectado a BigQuery


Se empleÃ³:

- 1 Cuenta de servicio (Service Account)

- 2 ConexiÃ³n segura por credenciales JSON

- 3 Modelo importado/DirectQuery segÃºn necesidad

Esto garantiza acceso controlado a datasets refinados sin exponer usuarios finales a los servicios de GCP.

ğŸ“˜ Para mÃ¡s detalle, revisa [07_PowerBI](07_PowerBI/README.md)

### âœ”ï¸ Seguridad, IAM, roles y gobierno

La seguridad fue implementada mediante IAM granular por miembro del equipo y por servicio, asignando permisos mÃ­nimos necesarios para cada flujo.

ğŸ“˜ Para mÃ¡s detalle, revisa [01_Ambiente_GCP](01_Ambiente_GCP/README.md)

## ğŸ” 2. Seguridad, IAM, Redes y Gobernanza 

### âœ”ï¸ IAM y roles 

Tal como se desarrollo en [01_Ambiente_GCP](01_Ambiente_GCP/README.md), se establecieron roles y claves de acceso para la analÃ­tica y contrucion de reportes.

Los roles para cada usuario son ingresados a travez del CLI de GCP, estos estran granularizados en:


- Roles generales : Son los roles otorgados en general al proyecto
- Roles para Scraping y carga de datos : Permite el uso de servicios como Cloud Run, Cloud Funtions y la creaciÃ³n de buckets para guardar los datos
- Roles para Procesamiento y ETL: Permite la creacion de recursos dataprocserverles para procesar los recursos a travez de las capas bronce, plata y oro creando la tabla de hechos final.

![Adicion_De_Roles](Evidencias_generales/2-Roles_IAM.png)

### âœ”ï¸ Registro de Logs

Para la observabilidad, el servicio de Loggins de GCP esta activado por defecto. Aqui se puede visualizar las fallas generadas en la ejecuciÃ³n de flujos desarrollardos por Cloud Run, Cloud Functions, BigQuery, Jobs, entre otros.

![Logging](Evidencias_generales/3-Observabilidad.png)


