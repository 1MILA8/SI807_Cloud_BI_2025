#  ğŸ¦ Proyecto BI Scotiabank 
## ğŸ—ï¸ PC4 â€“ Arquitectura AnalÃ­tica en la Nube con Google Cloud Platform
El objetivo de este trabajo es implementar un flujo funcional de sistema de inteligencia de Negocio en la nube para la empresa Scotiabank que permitira:
- âœ” Obtener datos desde la web
- âœ” Procesarlos y almacenarlos en un Data Lake multicapa
- âœ” Transformarlos con PySpark
- âœ” Consolidar informaciÃ³n en BigQuery
- âœ” Generar un esquema estrella en BigQuery
- âœ” Visualizar dashboards en Power BI

InformaciÃ³n de implementaciÃ³n estructurada de recursos GCP en ğŸ‘‰ [IMPLEMENTACION.md](01_Ambiente_GCP/IMPLEMENTACION.md)


### Estructura de Carpetas

```json
/PC4
 â”œâ”€â”€ README.md                 # DocumentaciÃ³n principal (este archivo)
 â”œâ”€â”€ 01_Ambiente_GCP/          # CreaciÃ³n del proyecto, IAM, VPC, buckets, claves
 â”œâ”€â”€ 02_WebScraping/           # CÃ³digo Python para extracciÃ³n de datos
 â”œâ”€â”€ 03_ETL/                   # Limpieza, consolidaciÃ³n, CSV, carga autom.
 â”œâ”€â”€ 04_DataLake/              # Raw / Trusted / Refined en GCS
 â”œâ”€â”€ 05_Procesamiento_Spark/   # Dataproc, PySpark, notebooks
 â”œâ”€â”€ 06_BigQuery/              # Tablas, consultas SQL
 â”œâ”€â”€ 07_PowerBI/               # ConexiÃ³n con BigQuery + dashboards
 â”œâ”€â”€ Evidencias_Generales/     # Capturas, videos, PR, merges
 â””â”€â”€ docs/                     
```
## ğŸ§± 1. Arquitectura Avanzada en la Nube

Arquitectura desplegada sobre Google Cloud Platform (GCP) en el proyecto:

```
ID del Proyecto: grupo6-scotiabank
```

La soluciÃ³n integra servicios bÃ¡sicos, avanzados y complementarios para soportar un flujo completo de analÃ­tica de datos, desde la ingesta hasta el consumo en Power BI.

### âœ”ï¸ Servicios utilizados y evidencias de costos

Durante la implementaciÃ³n se hizo uso de los siguientes servicios en la nube:

| Servicio                 | Rol dentro de la Arquitectura                                       |
|--------------------------|----------------------------------------------------------------------|
| **Networking**           | ComunicaciÃ³n segura entre servicios, gestiÃ³n de VPC, subredes y rutas privadas |
| **BigQuery**             | Motor analÃ­tico central para consultas SQL, creaciÃ³n de datasets y capas trusted/refined |
| **BigQuery Reservation** | Reserva de slots para consultas de alto rendimiento y procesamiento optimizado |
| **Dataproc**             | Procesamiento distribuido mediante PySpark para ETL y preparaciÃ³n de datos |
| **Compute Engine**       | Nodo de soporte y ejecuciÃ³n de tareas auxiliares o procesos puntuales |
| **Cloud Storage**        | Data Lake multicapa (raw â†’ trusted â†’ refined) para almacenamiento estructurado y no estructurado |
| **Cloud Run**            | EjecuciÃ³n de servicios serverless para microcomponentes y automatizaciÃ³n |
| **Cloud Run Functions**  | Funciones event-driven para orquestaciÃ³n y tareas desencadenadas por eventos |
| **Cloud Build**          | ConstrucciÃ³n, empaquetado y despliegue automÃ¡tico de artefactos y pipelines |
                            

![1-facturacion_Servicios](Evidencias_generales\1-Facturacion_Actual.png)


### âœ”ï¸ Storage estructurado (raw / trusted / refined)

La estrutura actual de Bucket en Cloud Storage tiene la siguiente forma

![alt text](Evidencias_Generales/4-Bukckets.png)

El Bucket Principal como Data Lake es el de **"grupo6_scotiabank_bucket"**.

Por otro lado se tiene Buckets adicionales generados automÃ¡ticamente por los servicios y recursos utilizados:

- **dataproc-temp-southamerica-west1-..Dataproc** : Ãrea temporal donde Dataproc guarda metadatos, logs, intermediarios y staging

- **gcf-v2-sources-75587073872-southamerica-west1	Cloud Run / Cloud Functions** : Almacena el cÃ³digo fuente desplegado por funciones y servicios serverless, permitiendo versionamiento y redeploy

El Data Lake se organizÃ³ bajo la estructura recomendada para arquitecturas analÃ­ticas:
```
gs://grupo6_scotiabank_bucket/   
â”‚   â”œâ”€â”€ Capa Bronce/ # Datos originales tal como se ingresa (Capa Bronce, data/raw/SBS)
â”œâ”€â”€â”€â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ historico/  #Archivos Consolidados
â”‚   â”‚   â””â”€â”€ raw/   # Datos Extraidos del Web Scraping
â”‚   â”‚       â””â”€â”€ SBS/
â”‚   â”‚           â”œâ”€â”€ CREDITOS_SEGUN_SITUACION/
â”‚   â”‚           â”œâ”€â”€ DEPOSITOS/
â”‚   â”‚           â”œâ”€â”€ EEFF/
â”‚   â”‚           â”œâ”€â”€ PATRIMONIO_EFECTIVO/
â”‚   â”‚           â”œâ”€â”€ PATRIMONIO_REQUERIDO_RCG/
â”‚   â”‚           â””â”€â”€ RATIO_LIQUIDEZ/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ resources/
â”œâ”€â”€â”€â””â”€â”€ Capa Plata/ # Datos curados, limpios y tipificados (Capa Plata, trusted)
â””â”€â”€â”€â””â”€â”€ Capa Oro/   # Datos listos para explotaciÃ³n analÃ­tica (Power BI / BigQuery, refined)
    
```

![Buckets_Data_Lake](Evidencias_Generales/5-Buckets_Data_Lake.png)

ğŸ‘‰ Este reparto permite separar la capa principal de datos del cÃ³digo operacional, garantizando gobernanza y control de versiones.


### âœ”ï¸ Visor BI en la nube

El consumo analÃ­tico se realiza mediante:

Power BI conectado a BigQuery


Se empleÃ³:

- 1 Cuenta de servicio (Service Account)

- 2 ConexiÃ³n segura por credenciales JSON

- 3 Modelo importado/DirectQuery segÃºn necesidad

Esto garantiza acceso controlado a datasets refinados sin exponer usuarios finales a los servicios de GCP.

ğŸ“˜ Para mÃ¡s detalle, revisa [07_PowerBI](07_PowerBI/README.md)

### âœ”ï¸ Seguridad, IAM, roles y gobierno

La seguridad fue implementada mediante IAM granular por miembro del equipo y por servicio, asignando permisos mÃ­nimos necesarios para cada flujo.

ğŸ“˜ Para mÃ¡s detalle, revisa [01_Ambiente_GCP](01_Ambiente_GCP/README.md)

## ğŸ” 2. Seguridad, IAM, Redes y Gobernanza 

### âœ”ï¸ IAM y roles 

Tal como se desarrollo en [01_Ambiente_GCP](01_Ambiente_GCP/README.md), se establecieron roles y claves de acceso para la analÃ­tica y contrucion de reportes.

Los roles para cada usuario son ingresados a travez del CLI de GCP, estos estran granularizados en:


- Roles generales : Son los roles otorgados en general al proyecto
- Roles para Scraping y carga de datos : Permite el uso de servicios como Cloud Run, Cloud Funtions y la creaciÃ³n de buckets para guardar los datos
- Roles para Procesamiento y ETL: Permite la creacion de recursos dataprocserverles para procesar los recursos a travez de las capas bronce, plata y oro creando la tabla de hechos final.

![Adicion_De_Roles](Evidencias_generales/2-Roles_IAM.png)

### âœ”ï¸ Registro de Logs

Para la observabilidad, el servicio de Loggins de GCP esta activado por defecto. Aqui se puede visualizar las fallas generadas en la ejecuciÃ³n de flujos desarrollardos por Cloud Run, Cloud Functions, BigQuery, Jobs, entre otros.

![Logging](Evidencias_generales/3-Observabilidad.png)


## ğŸ§© 5. Consultas SQL y ValidaciÃ³n del Modelo de Riesgo

Esta secciÃ³n documenta las validaciones ejecutadas sobre la tabla central de hechos hecho_riesgo y sus dimensiones asociadas dentro del Data Warehouse desarrollado en Google BigQuery.

ğŸ“˜ Evidencia de ValidaciÃ³n en [06_BigQuery](06_BigQuery/README.md).

### ğŸ” 5.1 ValidaciÃ³n de Calidad de Datos


#### **(Script 1) ValidaciÃ³n de valores nulos crÃ­ticos**
- Verifica la existencia de registros incompletos en las claves de negocio y campos numÃ©ricos.  
**Objetivo:** asegurar que no existan hechos sin referencia dimensional.
![Evidencia_SQL1](06_BigQuery/Evidencias/1-SQL-Script1.png)



#### **(Script 2) ValidaciÃ³n semÃ¡ntica contra lÃ­mites definidos**
- Clasifica cada valor como **Ã“ptimo / Amarillo / Riesgo** segÃºn los umbrales definidos para cada indicador.  
**Objetivo:** comprobar que los datos se interpretan correctamente antes de alimentar visualizaciones como semÃ¡foros o tacÃ³metros en Power BI.

![Evidencia_SQL2](06_BigQuery/Evidencias/1-SQL-Script2.png)


#### **(Script 3) ValidaciÃ³n de cobertura temporal del dataset**
- Revisa que cada indicador tenga registros en mÃºltiples periodos, evitando series incompletas.  
**Objetivo:** garantizar que los anÃ¡lisis evolutivos no tengan huecos que distorsionen el anÃ¡lisis.

![Evidencia_SQL3](06_BigQuery/Evidencias/1-SQL-Script3.png)


### ğŸ“Š 5.2 KPIs y MÃ©tricas Financieras

#### **(Script 4) KPI â€” Promedio histÃ³rico del indicador por banco**
- Calcula el valor medio de cada indicador por entidad bancaria.  
**Objetivo:** ofrecer una referencia sÃ³lida para evaluar el comportamiento relativo de cada banco.

![Evidencia_SQL4](06_BigQuery/Evidencias/1-SQL-Script4.png)


#### **(Script 5) KPI â€” Tendencia mensual (variaciÃ³n porcentual)**
- Analiza la evoluciÃ³n del valor del indicador mes a mes usando funciones de ventana (`LAG`).  
**Objetivo:** identificar si la situaciÃ³n financiera del banco mejora o empeora en el tiempo.  

![Evidencia_SQL5](06_BigQuery/Evidencias/1-SQL-Script5.png)

#### **(Script 6) KPI â€” Consolidado de salud financiera por banco**
- Resume la cantidad de indicadores en zonas **verde**, **amarilla** y **roja** para cada entidad financiera.  
**Objetivo:** facilitar una visiÃ³n global del riesgo institucional y priorizar acciones preventivas.

![Evidencia_SQL6](06_BigQuery/Evidencias/1-SQL-Script6.png)


Los resultados obtenidos a partir de estos scripts se encuentran documentados mediante:

âœ” Tablas con resultados visibles  
âœ” Video demostrativo del proceso  
âœ” Scripts SQL subidos al repositorio


Con la ejecuciÃ³n validada de los scripts:

| CategorÃ­a | Scripts | Resultado |
|----------|---------|-----------|
| Calidad de datos | 1â€“3 | Dataset Ã­ntegro y consistente |
| KPIs financieros | 4â€“6 | MÃ©tricas confiables para anÃ¡lisis |

