# ğŸ“ **Procesamiento_Spark - Capa Plata y Capa Oro (Dataproc)**

Aqui se explica los scripts PySpark encargados de procesar los datos desde la **capa Bronce (BigQuery)** hacia:

* **Capa Plata** â†’ Datos limpios, estandarizados, consolidados
* **Capa Oro** â†’ Tablas de hechos y dimensiones para anÃ¡lisis avanzado

Los procesos de esta carpeta se ejecutan automÃ¡ticamente despuÃ©s de los pipelines de ETL, mediante **Dataproc Serverless**.

---

### ğŸ“Œ **Recuerda: los scripts PySpark se encuentran en la carpeta**

ğŸ‘‰ **`/resources/jobs/`** ([Ver carpeta en GitHub](../resources/))

```
resources/
â””â”€â”€ jobs/
    â”œâ”€â”€ crear_capa_oro.py
    â”œâ”€â”€ jb_creditos.py
    â”œâ”€â”€ jb_depositos.py
    â”œâ”€â”€ jb_patrimonio.py
    â”œâ”€â”€ jb_ratio_liquidez.py
    â””â”€â”€ jb_tipo_cambio.py
```

Cada uno corresponde a una entidad procesada en la arquitectura Medallion.

---

# ğŸ¯ **Â¿QuÃ© hace exactamente?**

La misiÃ³n de esta carpeta es ejecutar, mediante PySpark, todos los procesos que requieren **procesamiento distribuido**, especÃ­ficamente:

| Capa      | DescripciÃ³n                                                                           |
| --------- | ------------------------------------------------------------------------------------- |
| **Plata** | Limpieza profunda, normalizaciÃ³n, tipificaciÃ³n, enriquecimiento                       |
| **Oro**   | CÃ¡lculo de indicadores, construcciÃ³n de tablas de hechos, integraciÃ³n con dimensiones |

Los jobs leen desde **Bronce (BigQuery)**, procesan con Spark y escriben en:

* `plata.<tabla>`
* `oro.<tabla>`

---

# ğŸ—‚ **Arquitectura del flujo Plata â†’ Oro**

## **âš¡Recordatorio: Este procesamiento es automÃ¡tico**

Solo se debe ejecutar el bronce-dispatcher, mas detalle en  **`/resources/`** ([Ver carpeta en GitHub](../resources/README.md))

![bronce-dispatcher](Evidencias/bronce-dispatcher.png)


AsÃ­ funciona:

1. **ğŸ“¤ Un archivo llega al bucket GCS**

2. **â˜ï¸ Cloud Function Gen2 (`bronce-dispatcher`) se dispara automÃ¡ticamente**

3. La funciÃ³n:

   * Detecta el tipo de archivo
   * Ejecuta la lÃ³gica ETL
   * Carga los datos a **Bronce**
   * Identifica quÃ© job Spark debe correr
   * **Lanza automÃ¡ticamente el contenido de `/05_Procesamiento_Spark/` en Dataproc**

4. **ğŸ§  Dataproc Serverless ejecuta el PySpark correspondiente**

5. El job:

   * Lee desde **Bronce**
   * Procesa **Plata**
   * Genera **Oro**
   * Escribe en BigQuery

ğŸ‘‰ *No ejecuta Dataproc, todo es event-driven (basado en eventos).*

### ğŸš€ **Â¿CÃ³mo se lanza un job automÃ¡ticamente?**

Cuando llega un archivo al bucket:

```
grupo6_scotiabank_bucket/
    â””â”€â”€ sbs_liquidez_2024_01.xlsx   â† evento detectado
```

La Cloud Function:

1. Identifica que pertenece al proceso **ratio de liquidez**
2. Mueve o procesa el archivo
3. Carga Bronce
4. Ejecuta:

```python
gcloud dataproc batches submit pyspark \
    gs://grupo6_scotiabank_bucket/resources/jb_ratio_liquidez.py \
    --region=southamerica-west1 \
    ...
```
Los jobs previamente se encuentran guardados en un bucket de GCS:

![Jobs en GCS](Evidencias/jobs_en_bucket.png)


Es decir:

ğŸ”´ **NO lo dispara el usuario**
ğŸŸ¢ **LO DISPARA Google Cloud Function automÃ¡ticamente**


### **Flujo COMPLETO del pipeline automÃ¡tico**

```mermaid
flowchart TD
    A[ğŸ“¥ Archivo subido al bucket GCS] --> B[âš¡ Cloud Function Gen2]
    B --> C[ğŸ—‚ ETL â†’ Bronce]
    C --> D[â˜ï¸ Cloud Function decide el JOB a ejecutar]
    D --> E[ğŸš€ Dataproc Serverless ejecuta PySpark]
    E --> F[ğŸ¥ˆ Capa Plata]
    F --> G[ğŸ¥‡ Capa Oro]
    G --> H[ğŸ“Š BigQuery listo para analytics]
```

---

## ğŸ§© **Ejemplo detallado: Job PySpark para Ratio de Liquidez**

El job completo es este:

> `resources/jobs/jb_ratio_liquidez.py`

A continuaciÃ³n se explica su funcionamiento paso a paso.

---

### **1ï¸âƒ£ InicializaciÃ³n del entorno Spark**

```python
spark = SparkSession.builder.appName("BroncePlataOro").enableHiveSupport().getOrCreate()
```

* Configura el job para ejecutarse en **Dataproc Serverless**
* Habilita soporte Hive (para BigQuery Connector)


Cada vez que se ejecuta, la Cloud Function dispara automÃ¡ticamente un batch de Dataproc para ejecutar el job correspondiente:

```
jb-ratio-liquidez-20250129-154233
```

Esta nomenclatura sigue el patrÃ³n:

```
jb-ratio-liquidez-YYYYMMDD-HHMMSS
```

Lo que permite identificar **exactamente cuÃ¡ndo fue lanzado el proceso** y facilita auditorÃ­a, debugging y trazabilidad.

---

### **2ï¸âƒ£ ParÃ¡metros globales**

```python
project_id = "grupo6-scotiabank"
dataset_plata = "plata"
dataset_oro = "oro"
table_plata = "sbs_liquidez"
table_oro = "hecho_riesgo"
```

---

### **3ï¸âƒ£ VerificaciÃ³n del dataset Plata**

```python
create_dataset_if_not_exists(dataset_plata)
```

Si el dataset *plata* no existe â†’ se crea automÃ¡ticamente.

---

### **4ï¸âƒ£ Lectura de la tabla Bronce**

```python
df_liquidez = spark.read.format("bigquery").option(
  "table", "grupo6-scotiabank.bronce.sbs_liquidez"
).load()
```

Luego filtra la data cargada recientemente:

```python
df_liquidez = df_liquidez.filter(
  F.col("fecha_carga") >= F.current_timestamp() - F.expr(f"INTERVAL {minutos} MINUTE")
)
```

---

### **5ï¸âƒ£ Limpieza y normalizaciÃ³n**

#### ğŸ”„ NormalizaciÃ³n de nombres de columnas

Elimina tildes y estandariza nombres:

```python
df_liquidez = df_liquidez.toDF(*[c.lower() for c in df_liquidez.columns])
```

#### ğŸ¦ NormalizaciÃ³n de instituciones

Ejemplo:

```python
when(lower(col("institucion")).like("%crÃ©dito%"), "BCP")
...
.when(lower(col("institucion")).like("%scotiabank%"), "Scotiabank")
```

#### ğŸ“… Normalizar meses

Mapeo abreviatura â†’ nÃºmero:

```python
mes_map = {"en": "01", "fe": "02", ...}
```

---

### ğŸ¥ˆ **6ï¸âƒ£ ConstrucciÃ³n de la Capa Plata**

```python
df_plata = df_filtered.select(
    to_date(concat_ws("-", col("anio"), lpad(col("mes"), 2, "0"), lit("01")), "yyyy-MM-dd").alias("fecha"),
    col("anio"),
    col("mes").cast(IntegerType()),
    col("institucion"),
    col("activos_liquidos"),
    col("pasivos_liquidos"),
    col("moneda")
)
```

Luego se guarda en:

```
plata.sbs_liquidez
```

---

### ğŸ¥‡ **7ï¸âƒ£ ConstrucciÃ³n de la Capa Oro (Modelo AnalÃ­tico)**

#### ğŸ“Œ Mapeo de bancos â†’ IDs

```python
bancos_map = {
    "Scotiabank": 1,
    "BCP": 2,
    "BBVA": 3,
    "Interbank": 4,
    ...
}
```

#### ğŸ“… UniÃ³n con `dim_fecha`

```python
df_ratio_prep.join(df_fecha, ["anio","mes"])
```

#### ğŸ§® CÃ¡lculo del indicador

```python
df_ratio_prep = df_ratio_prep.withColumn(
    "valor", (col("numerador") / col("denominador")).cast("double")
)
```

#### ğŸ“¤ Carga final en Oro

Finalmente, se escribe en:

```
oro.hecho_riesgo
```

### Batch de job_ratio_liquidez:

![batch-liquidez](Evidencias/batch_job_ratio_liquidez.png)

* **Logs representativos** exportados desde Cloud Logging (en formatos CSV y JSON), Ãºtiles para validar ejecuciones, estados del batch y mensajes del driver/worker.

Estos elementos permiten demostrar el funcionamiento correcto del pipeline y apoyar procesos de revisiÃ³n y monitoreo. [**Ver Logs**](Evidencias/Logs/jb_ratio_liquidez/)

---

# ğŸ—ï¸ **Ejecutar job especial: crear estructura Oro**

Este script crea dimensiones + estructura del modelo:

```bash
gcloud dataproc batches submit pyspark \
  gs://grupo6_scotiabank_bucket/resources/crear_capa_oro.py \
  --project=grupo6-scotiabank \
  --region=southamerica-west1 \
  --batch=create-oro-tables \
  --version=2.1 \
  --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar \
  --properties=spark.executor.instances=2,spark.executor.cores=4,spark.executor.memory=4g,spark.driver.cores=4,spark.driver.memory=4g
```

ğŸ‘‰ MÃ¡s detalle en **`/resources/`** ([Ver carpeta en GitHub](../resources/README.md))
ğŸ‘‰ [**Ver Logs**](Evidencias/Logs/crear_capa_oro/)


---
